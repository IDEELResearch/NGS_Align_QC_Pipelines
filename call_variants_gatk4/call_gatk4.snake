# /usr/bin/env python3
"""
call_gatk4.snake
Implement GATKv4 HaplotypeCaller 'incremental' pipeline: per-sample GVCFs first, then final merge for joint calling.
Do this in chunks to allow parallelization over independent loci.
"""

import os
import sys
from collections import OrderedDict
import pybedtools as pbt

## load regions into dict from a BedTool
def load_regions(bt, outdir):
	regions = OrderedDict()
	files = OrderedDict()
	for rr in bt:
		rname = "all"
		if rr.name and rr.name != ".":
			rname = rr.name
		if rname not in regions:
			regions[rname] = []
		regions[rname].append(rr)
	for rname in regions.keys():
		files[rname] = os.path.join(outdir, rname + ".bed")
	return regions, files

## load samples from file
def load_samples(f):
	s, b = [], []
	with open(f, "r") as ff:
		for line in ff:
			if line.startswith("#"):
				continue
			bam_entry = line.strip().split().pop(0)
			folder, bn = os.path.split(bam_entry)
			iid = os.path.splitext(bn)[0]
			s.append(iid)
			b.append(bam_entry)
	return s, dict(zip(s,b))

## read configuration stuff

# name of subdirectory where coordinates of target regions will be written
IVLDIR = "regions"
# a *.bed file of target regions -- each line a tuple of (chr,start,end,name) -- where intervals with the same
#	value in the 'name' field are part of the same 'chunk'. This controls how many parallel jobs are running.
#	For WGS, could split genome into ~10-50 Mb chunks. For WES, one chunk per chromosome seems to work well.
#	Note that this file should be sorted in genome order.
REGIONS = os.path.expandvars(config["regions"])
# list of *.bam files to process, which are assumed to be named like {sample}.bam
SAMPLES = os.path.expandvars(config["samples"])
# max number of ALT alleles per locus
MAXALLELES = config["max_alleles"]
# root directory for pipeline outputs
OUTDIR = os.path.expandvars(config["outdir"])
# temporary directory used by Java; probably should point to scratch space
TMPDIR = os.path.expandvars(config["tmpdir"])
# reference genome
REF = os.path.expandvars(config["ref"])

if not "mode" in config:
	config["mode"] = "novel"
else:
	if config["mode"] == "known":
		KNOWN_SITES = os.path.expandvars(config["known_sites"])
	elif config["mode"] == "novel":
		pass
	else:
		sys.exit(1)

## get list of samples and chunks to call
samples, bam_list = load_samples(SAMPLES)
regions, interval_files = load_regions(pbt.BedTool(REGIONS), IVLDIR)
#print(bam_list)
#print(regions.keys(), interval_files)

## make lists of chunk-wise intermediate files
gvcfs_by_chunk = OrderedDict()
for rname in regions.keys():
	gvcfs_by_chunk[rname] = expand(os.path.join(OUTDIR, "chunks/{region}/{sample}.g.vcf.gz"), sample = samples, region = rname)
#print(gvcfs_by_chunk)

## other intermediate targets
combined_gvcfs = expand(os.path.join(OUTDIR, "chunks/{region}/combined.g.vcf.gz"), region = regions.keys())

if config["mode"] == "novel":
	raw_vcfs = expand(os.path.join(OUTDIR, "chunks/{region}/firstpass.vcf.gz"), region = regions.keys())
	final_target = os.path.join(OUTDIR, "all_raw.vcf.gz")
elif config["mode"] == "known":
	raw_vcfs = expand(os.path.join(OUTDIR, "chunks/{region}/merged.vcf.gz"), region = regions.keys())
	final_target = os.path.join(OUTDIR, "all_genotyped.vcf.gz")

filtered_vcfs = expand(os.path.join(OUTDIR, "chunks/{region}/filtered.vcf.gz"), region = regions.keys())
vcf_list = os.path.join(OUTDIR, "chunks/all_vcfs.list")

## DONE
rule all:
	input:
		os.path.join(OUTDIR, "filtered.vcf.gz"),
		final_target

## final tidy-up
rule tidy_up:
	input:
		os.path.join(OUTDIR, "firstpass.vcf.gz")
	output:
		os.path.join(OUTDIR, "filtered.vcf.gz")
	params:
		ref = REF,
		memory = str(int(config["mem"]))
	shell:
		r"""
		gatk --java-options "-Xmx{params.memory}g" SelectVariants \
			-R {params.ref} \
			-V {input} \
			--set-filtered-gt-to-nocall \
			--exclude-filtered \
			--remove-unused-alternates \
			-O {output}
		"""

## combine into single file
rule concat_vcfs:
	input:
		vcfs = filtered_vcfs,
		flist = vcf_list
	output: os.path.join(OUTDIR, "firstpass.vcf.gz")
	shell:
		r"""
		bcftools concat -Oz {input.vcfs} >{output} && bcftools index --tbi {output}
		"""

## make text file with list of per-chunk annotated, filtered VCFs
rule make_vcf_list:
	input: filtered_vcfs
	output: vcf_list
	run:
		with open(vcf_list, "w") as lfile:
			for vcf in filtered_vcfs:
				print(vcf, file = lfile)

## add entries to FILTER per GATK4 hard-filtering best practices; don't actually apply the filter
# see https://informatics.fas.harvard.edu/whole-genome-resquencing-for-population-genomics-fastq-to-vcf.html#variantcalling
rule filter_vars:
	input:
		os.path.join(OUTDIR, "chunks/{region}/firstpass.vcf.gz")
	output:
		os.path.join(OUTDIR, "chunks/{region}/filtered.vcf.gz")
	params:
		ref = REF,
		memory = str(int(config["mem"])),
		min_DP = config["min_DP"],
		max_DP = config["max_DP"],
		min_RPRS_SNV = config["min_RPRS_SNV"],
		min_RPRS_indel = config["min_RPRS_indel"],
		min_QD = config["min_QD"],
		max_FS_SNV = config["max_FS_SNV"],
		max_SOR_SNV = config["max_SOR_SNV"],
		max_FS_indel = config["max_FS_indel"],
		max_SOR_indel = config["max_SOR_indel"],
		min_MQ = config["min_MQ"],
		min_MQRS = config["min_MQRS"]
	shell:
		r"""
		gatk --java-options "-Xmx{params.memory}g" VariantFiltration \
			-R {params.ref} \
			-V {input} \
			--invalidate-previous-filters \
			--genotype-filter-expression "!vc.hasAttribute('DP')" \
			--genotype-filter-name "NoDP" \
			--genotype-filter-expression "vc.hasAttribute('DP') && DP < {params.min_DP}" \
			--genotype-filter-name "MinDP" \
			--genotype-filter-expression "vc.hasAttribute('DP') && DP > {params.max_DP}" \
			--genotype-filter-name "MaxDP" \
			--filter-expression "(vc.hasAttribute('QD') && QD < {params.min_QD})" \
			--filter-name "LowQD" \
			--filter-expression "(vc.isSNP() && (vc.hasAttribute('ReadPosRankSum') && ReadPosRankSum < {params.min_RPRS_SNV})) || ((vc.isIndel() || vc.isMixed()) && (vc.hasAttribute('ReadPosRankSum') && ReadPosRankSum < {params.min_RPRS_indel})) " \
			--filter-name "PosBias" \
			--filter-expression "(vc.isSNP() && ((vc.hasAttribute('FS') && FS > {params.max_FS_SNV}) || (vc.hasAttribute('SOR') &&  SOR > {params.max_SOR_SNV}))) || ((vc.isIndel() || vc.isMixed()) && ((vc.hasAttribute('FS') && FS > {params.max_FS_indel}) || (vc.hasAttribute('SOR') &&  SOR > {params.max_SOR_indel})))" \
			--filter-name "StrandBias" \
			--filter-expression "vc.isSNP() && ((vc.hasAttribute('MQ') && MQ < {params.min_MQ}) || (vc.hasAttribute('MQRankSum') && MQRankSum < {params.min_MQRS}))" \
			--filter-name "LowMQ" \
			-O {output}
		"""

## concatenate chunks into UNFILTERED single file for VQSR
rule concat_raw_vcfs:
	input:
		vcfs = raw_vcfs
	output:
		final_target
	shell:
		r"""
		bcftools concat -Oz {input} >{output} && bcftools index --tbi {output}
		"""

## DISCOVERY MODE: perform joint calling chunk-wise
rule joint_genotype:
	input:
		combined_gvcf = os.path.join(OUTDIR, "chunks/{region}/combined.g.vcf.gz")
	output:
		os.path.join(OUTDIR, "chunks/{region}/firstpass.vcf.gz")
	params:
		maxalleles = MAXALLELES,
		ref = REF,
		interval_file = lambda w: interval_files[w.region],
		memory = str(int(config["mem"]))
	shell:
		r"""
		gatk --java-options "-Xmx{params.memory}g" GenotypeGVCFs \
			-R {params.ref} \
			-V {input.combined_gvcf} \
			-O {output} \
			-L {params.interval_file} \
			--max-alternate-alleles {params.maxalleles}
		"""

## DISCOVERY MODE: make chunk-wise GVCFs in prep for joint calling
rule combine_gvcfs:
	input:
		gvcfs = lambda w: gvcfs_by_chunk[w.region],
		gvcf_list = os.path.join(OUTDIR, "chunks/{region}/gvcfs.list")
	output:
		combined_gvcf = os.path.join(OUTDIR, "chunks/{region}/combined.g.vcf.gz")
	params:
		ref = REF,
		memory = str(int(config["mem"]))
	shell:
		r"""
		gatk --java-options "-Xmx{params.memory}g" CombineGVCFs \
			-R {params.ref} \
			--variant {input.gvcf_list} \
			-O {output.combined_gvcf}
		"""

## GENOTYPING MODE: simply merge chunk-wise VCFs
rule merge_gvcfs:
	input:
		gvcfs = lambda w: gvcfs_by_chunk[w.region],
		gvcf_list = os.path.join(OUTDIR, "chunks/{region}/gvcfs.list")
	output:
		combined_gvcf = os.path.join(OUTDIR, "chunks/{region}/merged.vcf.gz")
	params:
		ref = REF
	shell:
		r"""
		bcftools merge --file-list {input.gvcf_list} -Oz >{output} && bcftools index --tbi {output}
		"""

## make lists of GVCF files by chunk
rule make_gvcf_lists:
	input:
		lambda w: gvcfs_by_chunk[w.region]
	output:
		os.path.join(OUTDIR, "chunks/{region}/gvcfs.list")
	run:
		outfile = os.path.join(OUTDIR, "chunks/{}/gvcfs.list").format(wildcards.region)
		with open(outfile, "w") as list_file:
			for ff in gvcfs_by_chunk[wildcards.region]:
				print(ff, file = list_file)

## do first-pass variant calling per sample, in chunks
rule call_variants:
	input:
		intervals = interval_files.values(),
		bam = lambda w: bam_list[w.sample],
		ref = REF,
		known = KNOWN_SITES if config["mode"] == "known" else "."
	output:
		# output is organized by chunk, in anticipation of joing calling by chunk
		os.path.join(OUTDIR, "chunks/{region}/{sample}.g.vcf.gz")
	params:
		memory = str(int(config["mem"])),
		interval_file = lambda w: interval_files[w.region],
		mode = "GENOTYPE_GIVEN_ALLELES" if config["mode"] == "known" else "DISCOVERY",
		known = KNOWN_SITES if config["mode"] == "known" else "null",
		ref_conf = "NONE" if config["mode"] == "known" else "GVCF"
	shell:
		r"""
		gatk --java-options "-Xmx{params.memory}g" HaplotypeCaller \
			-R {input.ref} \
			-I {input.bam} \
			-O {output} \
			-L {params.interval_file} \
			--genotyping-mode {params.mode} \
			--alleles {params.known} \
			-ERC {params.ref_conf}
		"""

## split master target list into one file of intervals per chunk
rule make_interval_files:
	input:
		REGIONS
	output:
		interval_files.values()
	run:
		for rname, ivls in regions.items():
			with open(os.path.join(IVLDIR, rname + ".bed"), "w") as rfile:
				for ivl in ivls:
					rfile.write(str(ivl))
